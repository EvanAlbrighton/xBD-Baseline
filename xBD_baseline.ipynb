{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLUIcK-lAtI1"
      },
      "source": [
        "#NOTES\n",
        "\n",
        "Provided is the script used to to generate a baseline modle for comparision with an opitmized model. \n",
        "The baseline model is based off the xView2 baseline availiable at: https://github.com/DIUx-xView/xView2_baseline/tree/768b95e93ae56936e205bef0444137269a719755 though heavily modified.\n",
        "It should be noted that as this research is not concerned with building detection and rather building classification, the locaiztion model has not been run however is functioning and instructions for use provided.In later secions, work arounds are provided if the locaisation model is not run.\n",
        "\n",
        "It should be noted that this code is very sensitve and requires the file directory to be set up in a specific way. For this reason, arguments when executing code must be changed if running locally.\n",
        "\n",
        "Finally, it is recommended that due to the size of the datasets and complexity of the model that this code is ran using GPU acceleration through google colab+ or google colab pro+ paid with a google drive of sufficient storage.\n",
        "\n",
        "The second model provided as part of this research will be of the same format in a seperate document to avoid issues and confusion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy49hRqJfpUt"
      },
      "source": [
        "#Auxiliary\n",
        "Used if cannot be run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlBJ95jJfgRE",
        "outputId": "8e704a85-7872-497f-b648-90087108bf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing compute_mean.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile compute_mean.py\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import six\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "\tfrom PIL import Image\n",
        "\tavailable = True\n",
        "except ImportError as e:\n",
        "\tavailable = False\n",
        "\t_import_error = e\n",
        "\n",
        "from chainer.dataset import dataset_mixin\n",
        "\n",
        "\n",
        "def _check_pillow_availability():\n",
        "\tif not available:\n",
        "\t\traise ImportError('PIL cannot be loaded. Install Pillow!\\n'\n",
        "\t\t\t\t\t\t  'The actual import error is as follows:\\n' +\n",
        "\t\t\t\t\t\t  str(_import_error))\n",
        "\n",
        "\n",
        "def _read_image_as_array(path, dtype):\n",
        "\tf = Image.open(path)\n",
        "\ttry:\n",
        "\t\timage = np.asarray(f, dtype=dtype)\n",
        "\tfinally:\n",
        "\t\t# Only pillow >= 3.0 has 'close' method\n",
        "\t\tif hasattr(f, 'close'):\n",
        "\t\t\tf.close()\n",
        "\treturn image\n",
        "\n",
        "\n",
        "class ImageDataset(dataset_mixin.DatasetMixin):\n",
        "\t\n",
        "\tdef __init__(self, paths, root='.', dtype=np.float32):\n",
        "\t\t_check_pillow_availability()\n",
        "\t\tif isinstance(paths, six.string_types):\n",
        "\t\t\twith open(paths) as paths_file:\n",
        "\t\t\t\tpaths = [path.rstrip() for path in paths_file]\n",
        "\t\tself._paths = paths\n",
        "\t\tself._root = root\n",
        "\t\tself._dtype = dtype\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self._paths)\n",
        "\n",
        "\tdef get_example(self, i):\n",
        "\t\tpath = os.path.join(self._root, self._paths[i])\n",
        "\t\timage = _read_image_as_array(path, self._dtype)\n",
        "\n",
        "\t\tif image.ndim == 2:\n",
        "\t\t\t# image is greyscale\n",
        "\t\t\timage = image[:, :, np.newaxis]\n",
        "\t\treturn image.transpose(2, 0, 1)\n",
        "\n",
        "\n",
        "def compute_mean(dataset):\n",
        "\tprint('compute mean image')\n",
        "\tsum_color = 0\n",
        "\tN = len(dataset)\n",
        "\tfor i, image in enumerate(dataset):\n",
        "\t\tsum_color += image.mean(axis=2, keepdims=False).mean(axis=1, keepdims=False)\n",
        "\t\tsys.stderr.write('{} / {}\\r'.format(i, N))\n",
        "\t\tsys.stderr.flush()\n",
        "\tsys.stderr.write('\\n')\n",
        "\treturn sum_color / N\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\tparser = argparse.ArgumentParser(description='Compute images mean array')\n",
        "\t\n",
        "\tparser.add_argument('dataset',\n",
        "\t\t\t\t\t\thelp='Path to training image-label list file')\n",
        "\tparser.add_argument('--root', '-R', default='.',\n",
        "\t\t\t\t\t\thelp='Root directory path of image files')\n",
        "\tparser.add_argument('--output', '-o', default='mean.npy',\n",
        "\t\t\t\t\t\thelp='path to output mean array')\n",
        "\targs = parser.parse_args()\n",
        "\n",
        "\tdataset = ImageDataset(args.dataset, args.root)\n",
        "\tmean = compute_mean(dataset)\n",
        "\n",
        "\tnp.save(args.output, mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfaLUt6Pg54x",
        "outputId": "e4145c1d-ab35-4bc4-827c-83d2819906f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compute mean image\n",
            "89 / 90\n"
          ]
        }
      ],
      "source": [
        "!python3 compute_mean.py \"/content/drive/MyDrive/spacenet_gt/dataSet/train.txt\" --root \"/content/drive/MyDrive/Model_data/xBD/mexico-earthquake/images\" --output \"/content/drive/MyDrive/xView2_baseline-master/weights\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile split_into_disasters.py\n",
        "from os import walk, path, makedirs\n",
        "from shutil import copy2 as cp\n",
        "\n",
        "\n",
        "def get_files(base_dir):\n",
        "    # Minmizing (halfing) list to just pre image files\n",
        "    base_dir = path.join(base_dir, \"images\")\n",
        "    files = [f for f in next(walk(base_dir))[2] if \"pre\" in f]\n",
        "\n",
        "    return files\n",
        "\n",
        "\n",
        "def move_files(files, base_dir, output_dir):\n",
        "    for filename in files:\n",
        "        disaster = filename.split(\"_\")[0]\n",
        "\n",
        "        # If the output directory and disater name do not exist make the directory\n",
        "        if not path.isdir(path.join(output_dir, disaster)):\n",
        "            makedirs(path.join(output_dir, disaster))\n",
        "\n",
        "        # Check if the images directory exists\n",
        "        if not path.isdir(path.join(output_dir, disaster, \"images\")):\n",
        "            # If not create it\n",
        "            makedirs(path.join(output_dir, disaster, \"images\"))\n",
        "\n",
        "        # Move the pre and post image to the images directory under the disaster name\n",
        "        cp(\n",
        "            path.join(base_dir, \"images\", filename),\n",
        "            path.join(output_dir, disaster, \"images\", filename),\n",
        "        )\n",
        "        post_file = filename.replace(\"_pre_\", \"_post_\")\n",
        "        cp(\n",
        "            path.join(base_dir, \"images\", post_file),\n",
        "            path.join(output_dir, disaster, \"images\", post_file),\n",
        "        )\n",
        "\n",
        "        # Check if the label directory exists\n",
        "        if not path.isdir(path.join(output_dir, disaster, \"labels\")):\n",
        "            # If not create it\n",
        "            makedirs(path.join(output_dir, disaster, \"labels\"))\n",
        "\n",
        "        pre_label_file = filename.replace(\"png\", \"json\")\n",
        "        # Move the pre and post label files to the labels directory under the disaster name\n",
        "        cp(\n",
        "            path.join(base_dir, \"labels\", pre_label_file),\n",
        "            path.join(output_dir, disaster, \"labels\", pre_label_file),\n",
        "        )\n",
        "        post_label_file = pre_label_file.replace(\"_pre_\", \"_post_\")\n",
        "        cp(\n",
        "            path.join(base_dir, \"labels\", post_label_file),\n",
        "            path.join(output_dir, disaster, \"labels\", post_label_file),\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"split_into_disasters.py: Splits files under a single directory (with images/ and labels/ into directory of disasters/images|labels for the base submission pipeline (copies files)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--input\",\n",
        "        required=True,\n",
        "        metavar=\"/path/to/dataset/train\",\n",
        "        help=\"Full path to the train (or any other directory) with /images and /labels\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output\",\n",
        "        required=True,\n",
        "        metavar=\"/path/to/output/xBD\",\n",
        "        help=\"Full path to the output root dataset directory, will create disaster/images|labels under this directory\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    files = get_files(args.input)\n",
        "    move_files(files, args.input, args.output)"
      ],
      "metadata": {
        "id": "JkcXQnp_sFty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGo5oiP70NGm"
      },
      "source": [
        "# Mount Drive and Check for GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6xU7A6DaZ1a",
        "outputId": "a893690a-4cef-4ecb-af86-181a5e7efd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHcgBd4XVmgP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"content/drive/MyDrive/Colab Notebooks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNk5ayNX_0py",
        "outputId": "1b7ca02e-c8f6-4e74-a52c-321e79ff1430"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 2042219924683720063\n",
              " xla_global_id: -1]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZpFXMiO0YJz"
      },
      "source": [
        "#Localisation Spacenet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG4bk5Gexr4i",
        "outputId": "5f31d09b-4755-4d10-d38f-b983ed4f0c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting localisation_model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localisation_model.py\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "\n",
        "from unet import UNet\n",
        "from dataset import LabeledImageDataset\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('dataset', help='Path to directory containing train.txt, val.txt, and mean.npy')\n",
        "    parser.add_argument('images',  help='Root directory of input images')\n",
        "    parser.add_argument('labels',  help='Root directory of label images')\n",
        "    \n",
        "    parser.add_argument('--batchsize', '-b', type=int, default=16,\n",
        "                        help='Number of images in each mini-batch')\n",
        "    parser.add_argument('--test-batchsize', '-B', type=int, default=4,\n",
        "                        help='Number of images in each test mini-batch')\n",
        "    parser.add_argument('--epoch', '-e', type=int, default=50,\n",
        "                        help='Number of sweeps over the dataset to train')\n",
        "    parser.add_argument('--frequency', '-f', type=int, default=1,\n",
        "                        help='Frequency of taking a snapshot')\n",
        "    parser.add_argument('--gpu', '-g', type=int, default=0,\n",
        "                        help='GPU ID (negative value indicates CPU)') #NOTE: function removed\n",
        "    parser.add_argument('--out', '-o', default='logs',\n",
        "                        help='Directory to output the result under \"models\" directory')\n",
        "    parser.add_argument('--resume', '-r', default='',\n",
        "                        help='Resume the training from snapshot')\n",
        "    parser.add_argument('--noplot', dest='plot', action='store_false',\n",
        "                        help='Disable PlotReport extension')\n",
        "\n",
        "    parser.add_argument('--tcrop', type=int, default=400,\n",
        "                        help='Crop size for train-set images')\n",
        "    parser.add_argument('--vcrop', type=int, default=480,\n",
        "                        help='Crop size for validation-set images')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    assert (args.tcrop % 16 == 0) and (args.vcrop % 16 == 0), \"tcrop and vcrop must be divisible by 16.\"\n",
        "\n",
        "\n",
        "    from tboard_logger import TensorboardLogger\n",
        "\n",
        "    print('GPU: {}'.format(args.gpu))\n",
        "    print('# Minibatch-size: {}'.format(args.batchsize))\n",
        "    print('# Crop-size: {}'.format(args.tcrop))\n",
        "    print('# epoch: {}'.format(args.epoch))\n",
        "    print('')\n",
        "    \n",
        "    this_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    models_dir = os.path.normpath(os.path.join(this_dir, \"../../models\"))\n",
        "    log_dir = os.path.join(models_dir, args.out)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    \n",
        "    # Set up a neural network to train\n",
        "    # Classifier reports softmax cross entropy loss and accuracy at every\n",
        "    # iteration, which will be used by the PrintReport extension below.\n",
        "    model = UNet()\n",
        "    if args.gpu >= 0:\n",
        "        # Make a specified GPU current\n",
        "        chainer.cuda.get_device_from_id(args.gpu).use()\n",
        "        model.to_gpu()  # Copy the model to the GPU\n",
        "\n",
        "    # Setup an optimizer\n",
        "    optimizer = chainer.optimizers.Adam()\n",
        "    optimizer.setup(model)\n",
        "    \n",
        "    # Load mean image\n",
        "    mean = np.load(os.path.join(args.dataset, \"mean.npy\"))\n",
        "    \n",
        "    # Load the MNIST dataset\n",
        "    train = LabeledImageDataset(os.path.join(args.dataset, \"train.txt\"), args.images, args.labels, \n",
        "                                mean=mean, crop_size=args.tcrop, test=False, distort=False)\n",
        "    \n",
        "    test = LabeledImageDataset (os.path.join(args.dataset, \"val.txt\"), args.images, args.labels, \n",
        "                                mean=mean, crop_size=args.vcrop, test=True, distort=False)\n",
        "\n",
        "    train_iter = chainer.iterators.SerialIterator(train, args.batchsize)\n",
        "    test_iter = chainer.iterators.SerialIterator(test, args.test_batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "    # Set up a trainer\n",
        "    updater = training.StandardUpdater(\n",
        "        train_iter, optimizer, device=args.gpu)\n",
        "    trainer = training.Trainer(updater, (args.epoch, 'epoch'), out=log_dir)\n",
        "\n",
        "    # Evaluate the model with the test dataset for each epoch\n",
        "    trainer.extend(extensions.Evaluator(test_iter, model, device=args.gpu))\n",
        "\n",
        "    # Dump a computational graph from 'loss' variable at the first iteration\n",
        "    # The \"main\" refers to the target link of the \"main\" optimizer.\n",
        "    trainer.extend(extensions.dump_graph('main/loss'))\n",
        "\n",
        "    # Take a snapshot for each specified epoch\n",
        "    frequency = args.epoch if args.frequency == -1 else max(1, args.frequency)\n",
        "    trainer.extend(extensions.snapshot(), trigger=(frequency, 'epoch'))\n",
        "    \n",
        "    # Save trained model for each specific epoch\n",
        "    trainer.extend(extensions.snapshot_object(\n",
        "        model, 'model_iter_{.updater.iteration}'), trigger=(frequency, 'epoch'))\n",
        "\n",
        "    # Write a log of evaluation statistics for each epoch\n",
        "    trainer.extend(extensions.LogReport())\n",
        "\n",
        "    # Save two plot images to the result dir\n",
        "    if args.plot and extensions.PlotReport.available():\n",
        "        trainer.extend(\n",
        "            extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
        "                                  'epoch', file_name='loss.png'))\n",
        "        trainer.extend(\n",
        "            extensions.PlotReport(\n",
        "                ['main/accuracy', 'validation/main/accuracy'],\n",
        "                'epoch', file_name='accuracy.png'))\n",
        "\n",
        "    # Print selected entries of the log to stdout\n",
        "    # Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
        "    # \"validation\" refers to the default name of the Evaluator extension.\n",
        "    # Entries other than 'epoch' are reported by the Classifier link, called by\n",
        "    # either the updater or the evaluator.\n",
        "    trainer.extend(extensions.PrintReport(\n",
        "        ['epoch', 'main/loss', 'validation/main/loss',\n",
        "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
        "\n",
        "    # Print a progress bar to stdout\n",
        "    trainer.extend(extensions.ProgressBar())\n",
        "    \n",
        "    # Write training log to TensorBoard log file\n",
        "    trainer.extend(TensorboardLogger(writer,\n",
        "        ['main/loss', 'validation/main/loss',\n",
        "         'main/accuracy', 'validation/main/accuracy']))\n",
        "    \n",
        "    if args.resume:\n",
        "        # Resume from a snapshot\n",
        "        chainer.serializers.load_npz(args.resume, trainer)\n",
        "\n",
        "    # Run the training\n",
        "    trainer.run()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ9A6lTRva6X",
        "outputId": "b626ec0d-715d-4e95-847e-dd8d634dd0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   8586      0 --:--:-- --:--:-- --:--:--  8586\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "+ pip install -q cupy-cuda100 ==5.1.0 chainer ==5.1.0\n",
            "\u001b[K     |████████████████████████████████| 298.6 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 513 kB 55.4 MB/s \n",
            "\u001b[?25h  Building wheel for chainer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "+ set +ex\n",
            "Installation succeeded!\n",
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.5\n"
          ]
        }
      ],
      "source": [
        "!curl https://colab.chainer.org/install | CHAINER_VERSION=\"==5.1.0\" CUPY_VERSION=\"==5.1.0\" sh -\n",
        "!pip install tensorboardx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkz8i9XG2dAZ",
        "outputId": "a4f78fee-30cd-480b-e96a-00059126e568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/xView2_baseline-master/spacenet/src/models\n"
          ]
        }
      ],
      "source": [
        "#Set base path for python to retrieve required accessories\n",
        "%cd \"/content/drive/MyDrive/xView2_baseline-master/spacenet/src/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ys3utpS4ZG4",
        "outputId": "393804c1-403f-4132-b8c6-8d4823728126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/xView2_baseline-master/spacenet/src/models\n"
          ]
        }
      ],
      "source": [
        "#Check returns path/to/spacenet/src/models\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKAq2Z69ywed"
      },
      "outputs": [],
      "source": [
        "!python3 localisation_model.py \"/content/drive/MyDrive/spacenet_gt/dataSet\" \"/content/drive/MyDrive/spacenet_gt/images\" \"/content/drive/MyDrive/spacenet_gt/labels\" -e 50 -o \"/content/drive/MyDrive/Model_data/localisation_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hogIePEm0loP"
      },
      "source": [
        "#Damage Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsleAZwl4dWm",
        "outputId": "d11e767f-ee19-452f-dc23-df004b92ae97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "#Reset path to base\n",
        "%cd \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBGBE4Sfjekz",
        "outputId": "634708ae-a731-4a26-8f0a-5d6a1ef01996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing process_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile process_data.py\n",
        "from PIL import Image\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import cv2\n",
        "import datetime\n",
        "\n",
        "import shapely.wkt\n",
        "import shapely\n",
        "from shapely.geometry import Polygon\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configurations\n",
        "NUM_WORKERS = 4\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 120\n",
        "LEARNING_RATE = 0.0001\n",
        "RANDOM_SEED = 123\n",
        "LOG_STEP = 150\n",
        "\n",
        "damage_intensity_encoding = defaultdict(lambda: 0)\n",
        "damage_intensity_encoding['destroyed'] = 3\n",
        "damage_intensity_encoding['major-damage'] = 2\n",
        "damage_intensity_encoding['minor-damage'] = 1\n",
        "damage_intensity_encoding['no-damage'] = 0\n",
        "\n",
        "\n",
        "def process_img(img_array, polygon_pts, scale_pct):\n",
        "    \"\"\"Process Raw Data into\n",
        "\n",
        "            Args:\n",
        "                img_array (numpy array): numpy representation of image.\n",
        "                polygon_pts (array): corners of the building polygon.\n",
        "\n",
        "            Returns:\n",
        "                numpy array: .\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    height, width, _ = img_array.shape\n",
        "\n",
        "    xcoords = polygon_pts[:, 0]\n",
        "    ycoords = polygon_pts[:, 1]\n",
        "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
        "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
        "\n",
        "    xdiff = xmax - xmin\n",
        "    ydiff = ymax - ymin\n",
        "\n",
        "    #Extend image by scale percentage\n",
        "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
        "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
        "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
        "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
        "\n",
        "    return img_array[ymin:ymax, xmin:xmax, :]\n",
        "\n",
        "\n",
        "def process_data(input_path, output_path, output_csv_path, val_split_pct):\n",
        "    \"\"\"Process Raw Data into\n",
        "\n",
        "        Args:\n",
        "            dir_path (path): Path to the xBD dataset.\n",
        "            data_type (string): String to indicate whether to process\n",
        "                                train, test, or holdout data.\n",
        "\n",
        "        Returns:\n",
        "            x_data: A list of numpy arrays representing the images for training\n",
        "            y_data: A list of labels for damage represented in matrix form\n",
        "\n",
        "    \"\"\"\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "\n",
        "    disasters = [folder for folder in os.listdir(input_path) if not folder.startswith('.')]\n",
        "    disaster_paths = ([input_path + \"/\" +  d + \"/images\" for d in disasters])\n",
        "    image_paths = []\n",
        "    image_paths.extend([(disaster_path + \"/\" + pic) for pic in os.listdir(disaster_path)] for disaster_path in disaster_paths)\n",
        "    img_paths = np.concatenate(image_paths)\n",
        "\n",
        "    for img_path in tqdm(img_paths):\n",
        "\n",
        "        img_obj = Image.open(img_path)\n",
        "        img_array = np.array(img_obj)\n",
        "\n",
        "        #Get corresponding label for the current image\n",
        "        label_path = img_path.replace('png', 'json').replace('images', 'labels')\n",
        "        label_file = open(label_path)\n",
        "        label_data = json.load(label_file)\n",
        "\n",
        "        for feat in label_data['features']['xy']:\n",
        "\n",
        "            # only images post-disaster will have damage type\n",
        "            try:\n",
        "                damage_type = feat['properties']['subtype']\n",
        "            except: # pre-disaster damage is default no-damage\n",
        "                damage_type = \"no-damage\"\n",
        "                continue\n",
        "\n",
        "            poly_uuid = feat['properties']['uid'] + \".png\"\n",
        "\n",
        "            y_data.append(damage_intensity_encoding[damage_type])\n",
        "\n",
        "            polygon_geom = shapely.wkt.loads(feat['wkt'])\n",
        "            polygon_pts = np.array(list(polygon_geom.exterior.coords))\n",
        "            poly_img = process_img(img_array, polygon_pts, 0.8)\n",
        "            cv2.imwrite(output_path + \"/\" + poly_uuid, poly_img)\n",
        "            x_data.append(poly_uuid)\n",
        "    \n",
        "    output_train_csv_path = os.path.join(output_csv_path, \"train.csv\")\n",
        "\n",
        "    if(val_split_pct > 0):\n",
        "       x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=val_split_pct)\n",
        "       data_array_train = {'uuid': x_train, 'labels': y_train}\n",
        "       data_array_test = {'uuid': x_test, 'labels': y_test}\n",
        "       output_test_csv_path = os.path.join(output_csv_path, \"test.csv\")\n",
        "       df_train = pd.DataFrame(data_array_train)\n",
        "       df_test = pd.DataFrame(data_array_test)\n",
        "       df_train.to_csv(output_train_csv_path)\n",
        "       df_test.to_csv(output_test_csv_path)\n",
        "    else: \n",
        "       data_array = {'uuid': x_data, 'labels': y_data}\n",
        "       df = pd.DataFrame(data = data_array)\n",
        "       df.to_csv(output_train_csv_path)\n",
        "    \n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run Building Damage Classification Training & Evaluation')\n",
        "    parser.add_argument('--input_dir',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_input\",\n",
        "                        help=\"Full path to the parent dataset directory\")\n",
        "    parser.add_argument('--output_dir',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/xBD_output',\n",
        "                        help=\"Path to new directory to save images\")\n",
        "    parser.add_argument('--output_dir_csv',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/xBD_output_csv',\n",
        "                        help=\"Path to new directory to save csv\")\n",
        "    parser.add_argument('--val_split_pct', \n",
        "                        required=False,\n",
        "                        default=0.0,\n",
        "                        metavar='Percentage to split validation',\n",
        "                        help=\"Percentage to split \")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    logging.info(\"Started Processing for Data\")\n",
        "    process_data(args.input_dir, args.output_dir, args.output_dir_csv, float(args.val_split_pct))\n",
        "    logging.info(\"Finished Processing Data\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        " \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSbdRuyMjeaY",
        "outputId": "b42c0248-5c75-4717-d35d-5545d91664e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:root:Started Processing for Data\n",
            "100% 240/240 [03:37<00:00,  1.10it/s]\n",
            "INFO:root:Finished Processing Data\n"
          ]
        }
      ],
      "source": [
        "!python3 process_data.py --input_dir \"/content/drive/MyDrive/Model_data/xBD\" --output_dir \"/content/drive/MyDrive/Model_data/test-train2\" --output_dir_csv \"/content/drive/MyDrive/Model_data/output_csv\" --val_split_pct .5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KKZIqEQCJsj"
      },
      "outputs": [],
      "source": [
        "#From the test.csv file, file names are copied into txt to be moved to test folder using code below. Repeated for training files.\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "file_source ='/content/drive/MyDrive/Model_data/test-train2'\n",
        "dst ='/content/drive/MyDrive/Model_data/test-train2/test'\n",
        "\n",
        "with open('/content/drive/MyDrive/Model_data/testset.txt') as my_file:\n",
        "    for filename in my_file:\n",
        "        src = os.path.join(file_source, filename.strip() )\n",
        "        os.rename(src, os.path.join(dst, filename.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8IR9Vk3P0tW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "file_source ='/content/drive/MyDrive/Model_data/test-train2'\n",
        "dst ='/content/drive/MyDrive/Model_data/test-train2/train'\n",
        "\n",
        "with open('/content/drive/MyDrive/Model_data/trainset.txt') as my_file:\n",
        "    for filename in my_file:\n",
        "        src = os.path.join(file_source, filename.strip() )\n",
        "        os.rename(src, os.path.join(dst, filename.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYe-_clMEDSw",
        "outputId": "62dc4ca2-e3b0-41b4-e625-fb2a13ec739f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing damage_classification.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile damage_classification.py\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import cv2\n",
        "import datetime\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import shapely.wkt\n",
        "import shapely\n",
        "from shapely.geometry import Polygon\n",
        "from collections import defaultdict\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import keras\n",
        "import ast\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "# Loss function for ordinal loss from https://github.com/JHart96/keras_ordinal_categorical_crossentropy\n",
        "###\n",
        "def ordinal_loss(y_true, y_pred):\n",
        "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
        "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )\n",
        "\n",
        "\n",
        "###\n",
        "# Generate a simple CNN\n",
        "###\n",
        "def generate_xBD_baseline_model():\n",
        "  weights = 'imagenet'\n",
        "  inputs = Input(shape=(128, 128, 3))\n",
        "\n",
        "  base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  base_resnet = base_model(inputs)\n",
        "  base_resnet = Flatten()(base_resnet)\n",
        "\n",
        "  concated_layers = Concatenate()([x, base_resnet])\n",
        "\n",
        "  concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
        "  concated_layers = Dense(524, activation='relu')(concated_layers)\n",
        "  concated_layers = Dense(124, activation='relu')(concated_layers)\n",
        "  output = Dense(3, activation='relu')(concated_layers)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configurations\n",
        "NUM_WORKERS = 1 \n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 30\n",
        "LEARNING_RATE = 0.0001\n",
        "RANDOM_SEED = 123\n",
        "LOG_STEP = 400\n",
        "LOG_DIR = '/path/to/logs' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "damage_intensity_encoding = dict()\n",
        "damage_intensity_encoding[3] = '3'\n",
        "damage_intensity_encoding[2] = '2' \n",
        "damage_intensity_encoding[1] = '1' \n",
        "damage_intensity_encoding[0] = '0' \n",
        "\n",
        "\n",
        "# Function to compute unweighted f1 scores, just for reference\n",
        "###\n",
        "\n",
        "\n",
        "def f1_unweighted(y_true, y_pred):\n",
        "    def recall_unweighted(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "        Only computes a batch-wise average of recall.\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives_un = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives_un = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall_unweighted = true_positives_un / (possible_positives_un + K.epsilon())\n",
        "        return recall_unweighted\n",
        "\n",
        "    def precision_unweighted(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "        Only computes a batch-wise average of precision.\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives_un = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives_un = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision_unweighted = true_positives_un / (predicted_positives_un + K.epsilon())\n",
        "        return precision_unweighted\n",
        "\n",
        "    precision_unweighted = precision_unweighted(y_true, y_pred)\n",
        "    recall_unweighted = recall_unweighted(y_true, y_pred)\n",
        "    return 2*((precision_unweighted*recall_unweighted)/(precision_unweighted+recall_unweighted+K.epsilon()))\n",
        "\n",
        "\n",
        "###\n",
        "# Creates data generator for validation set\n",
        "###\n",
        "def validation_generator(test_csv, test_dir):\n",
        "    df = pd.read_csv(test_csv)\n",
        "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
        "\n",
        "    gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                             rescale=1/255.)\n",
        "\n",
        "\n",
        "    return gen.flow_from_dataframe(dataframe=df,\n",
        "                                   directory=test_dir,\n",
        "                                   x_col='uuid',\n",
        "                                   y_col='labels',\n",
        "                                   batch_size=BATCH_SIZE,\n",
        "                                   shuffle=False,\n",
        "                                   seed=RANDOM_SEED,\n",
        "                                   class_mode=\"categorical\",\n",
        "                                   target_size=(128, 128))\n",
        "\n",
        "\n",
        "###\n",
        "# Applies random transformations to training data\n",
        "###\n",
        "def augment_data(df, in_dir):\n",
        "\n",
        "    df = df.replace({\"labels\" : damage_intensity_encoding })\n",
        "    gen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
        "                             vertical_flip=True,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             rescale=1/255.)\n",
        "    return gen.flow_from_dataframe(dataframe=df,\n",
        "                                   directory=in_dir,\n",
        "                                   x_col='uuid',\n",
        "                                   y_col='labels',\n",
        "                                   batch_size=BATCH_SIZE,\n",
        "                                   seed=RANDOM_SEED,\n",
        "                                   class_mode=\"categorical\",\n",
        "                                   target_size=(128, 128))\n",
        "\n",
        "\n",
        "# Run training and evaluation based on existing or new model\n",
        "def train_model(train_data, train_csv, test_data, test_csv, model_in, model_out):\n",
        "\n",
        "    model = generate_xBD_baseline_model()\n",
        "\n",
        "    # Add model weights if provided by user\n",
        "    if model_in is not None:\n",
        "        model.load_weights(model_in)\n",
        "\n",
        "    df = pd.read_csv(train_csv)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight = \"balanced\",\n",
        "        classes = np.unique(df['labels'].to_list()),\n",
        "        y = df['labels'].to_list());\n",
        "    d_class_weights = dict(enumerate(class_weights))\n",
        "    print('Class weights'+str(d_class_weights))\n",
        "\n",
        "    samples = df['uuid'].count()\n",
        "    steps = np.ceil(samples/BATCH_SIZE)\n",
        "\n",
        "    # Augments the training data\n",
        "    train_gen_flow = augment_data(df, train_data)\n",
        "\n",
        "    #Set up tensorboard logging\n",
        "    tensorboard_callbacks = keras.callbacks.TensorBoard(log_dir=LOG_DIR,\n",
        "                                                        batch_size=BATCH_SIZE)\n",
        "\n",
        "    \n",
        "    #Filepath to save model weights\n",
        "    filepath = model_out + \"-saved-model-{epoch:02d}-{accuracy:.2f}.hdf5\"\n",
        "    checkpoints = keras.callbacks.ModelCheckpoint(filepath,\n",
        "                                                    monitor=['loss', 'accuracy'],\n",
        "                                                    verbose=1,\n",
        "                                                    save_best_only=False,\n",
        "                                                    mode='max')\n",
        "\n",
        "    #Adds adam optimizer\n",
        "    adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE,\n",
        "                                    beta_1=0.9,\n",
        "                                    beta_2=0.999,\n",
        "                                    decay=0.0,\n",
        "                                    amsgrad=False)\n",
        "    \n",
        "        #Defines the metrics to be used\n",
        "    METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve        \n",
        "    ]\n",
        "    \n",
        "    #Defines the metrics to be used\n",
        "    model.compile(loss=ordinal_loss, optimizer=adam, metrics=['accuracy', f1_unweighted, METRICS])\n",
        "    model.summary\n",
        "\n",
        "    # Augments the training and validaton data\n",
        "    train_ds = augment_data(df, train_data)\n",
        "    validation_dataset = validation_generator(test_csv, test_data)\n",
        "\n",
        "    #Training begins\n",
        "    history = model.fit_generator(generator=train_ds,\n",
        "                        validation_data=validation_dataset,\n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        workers=NUM_WORKERS,\n",
        "                        use_multiprocessing=True,\n",
        "                        class_weight=d_class_weights,\n",
        "                        callbacks=[tensorboard_callbacks, checkpoints],\n",
        "                        verbose=1)\n",
        "\n",
        "\n",
        "    #Evalulate f1 weighted scores on validation set\n",
        "    validation_gen = validation_generator(test_csv, test_data)\n",
        "    predictions = model.predict(validation_gen)\n",
        "\n",
        "    val_trues = validation_gen.classes\n",
        "    val_pred = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    f1_weighted = f1_score(val_trues, val_pred, average='weighted')\n",
        "    print('validation f1 score: ' +str(f1_weighted))\n",
        "\n",
        "\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(NUM_EPOCHS)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run Building Damage Classification Training & Evaluation')\n",
        "    parser.add_argument('--train_data',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_train\",\n",
        "                        help=\"Full path to the train data directory\")\n",
        "    parser.add_argument('--train_csv',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_split\",\n",
        "                        help=\"Full path to the train csv\")\n",
        "    parser.add_argument('--test_data',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_test\",\n",
        "                        help=\"Full path to the test data directory\")\n",
        "    parser.add_argument('--test_csv',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_split\",\n",
        "                        help=\"Full path to the test csv\")\n",
        "    parser.add_argument('--model_in',\n",
        "                        default=None,\n",
        "                        metavar='/path/to/input_model',\n",
        "                        help=\"Path to save model\")\n",
        "    parser.add_argument('--model_out',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/save_model',\n",
        "                        help=\"Path to save model\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(args.train_data, args.train_csv, args.test_data, args.test_csv, args.model_in, args.model_out)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-H_uzbfYgis"
      },
      "outputs": [],
      "source": [
        "!python3 damage_classification.py --train_data \"/content/drive/MyDrive/Model_data/test-train\" --train_csv \"/content/drive/MyDrive/Model_data/output_csv/train_skewed.csv\" --test_data \"/content/drive/MyDrive/Model_data/test-train\" --test_csv \"/content/drive/MyDrive/Model_data/output_csv/test.csv\" --model_out \"/content/drive/MyDrive/Model_data/saved_models/pre_test/pre-recall\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvHGVVmYw_xP"
      },
      "source": [
        "#Inference\n",
        "\n",
        "Combing spacenet (localization) model and classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm42s8j6YeR1",
        "outputId": "49428797-6b92-4ef0-f050-2680c875408b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1580  100  1580    0     0   7348      0 --:--:-- --:--:-- --:--:--  7348\n",
            "+ apt -y -q install cuda-libraries-dev-10-0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "cuda-libraries-dev-10-0 is already the newest version (10.0.130-1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "+ pip install -q cupy-cuda100 ==5.1.0 chainer ==5.1.0\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "^C\n",
            "Collecting imantics\n",
            "  Downloading imantics-0.1.12.tar.gz (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imantics) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from imantics) (4.1.2.30)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from imantics) (4.2.6)\n",
            "Collecting xmljson\n",
            "  Downloading xmljson-0.2.1-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: imantics\n",
            "  Building wheel for imantics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imantics: filename=imantics-0.1.12-py3-none-any.whl size=16033 sha256=e9faf58f1db02d6cfd8991e9efd27dcdfbc630704df9f56ad1733a47b2fbb689\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/7c/3e/296fe3ed4eb3bd713e91dee0d0549f12f316d49939a64bdc96\n",
            "Successfully built imantics\n",
            "Installing collected packages: xmljson, imantics\n",
            "Successfully installed imantics-0.1.12 xmljson-0.2.1\n",
            "Collecting simplification\n",
            "  Downloading simplification-0.5.22-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (693 kB)\n",
            "\u001b[K     |████████████████████████████████| 693 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29.0 in /usr/local/lib/python3.7/dist-packages (from simplification) (0.29.28)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from simplification) (1.21.6)\n",
            "Installing collected packages: simplification\n",
            "Successfully installed simplification-0.5.22\n"
          ]
        }
      ],
      "source": [
        "!curl https://colab.chainer.org/install | CHAINER_VERSION=\"==5.1.0\" CUPY_VERSION=\"==5.1.0\" sh -\n",
        "!pip install imantics\n",
        "!pip install simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akOKwuVTIZlT",
        "outputId": "380c009f-0e27-44cb-ffb6-a347fe10aca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Grabbing post image file for classification\n",
            "Running classification\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Creating directories\n",
        "%%shell\n",
        "set -euo pipefail\n",
        "\n",
        "# this function is called when Ctrl-C is sent\n",
        "function trap_ctrlc ()\n",
        "{\n",
        "    # perform cleanup here\n",
        "    echo \"Ctrl-C or Error caught...performing clean up check /tmp/inference.log\"\n",
        "\n",
        "    if [ -d /tmp/inference ]; then\n",
        "           rm -rf /tmp/inference\n",
        "    fi\n",
        "\n",
        "    exit 99\n",
        "}\n",
        "\n",
        "# initialise trap to call trap_ctrlc function\n",
        "# when signal 2 (SIGINT) is received\n",
        "trap \"trap_ctrlc\" 2 9 13 3\n",
        "\n",
        "help_message () {\n",
        "        printf \"${0}: Runs the polygonization in inference mode\\n\\t-x: path to xview-2 repository\\n\\t-i: /full/path/to/input/pre-disaster/image.png\\n\\t-p: /full/path/to/input/post-disaster/image.png\\n\\t-o: /path/to/output.png\\n\\t-l: path/to/localization_weights\\n\\t-c: path/to/classification_weights\\n\\t-e /path/to/virtual/env/activate\\n\\t-y continue with local environment and without interactive prompt\\n\\n\"\n",
        "}\n",
        "\n",
        "input=\"/content/drive/MyDrive/mexico_pre/mexico-earthquake_00000002_pre_disaster.png\"\n",
        "input_post=\"/content/drive/MyDrive/mexico_post/mexico-earthquake_00000002_post_disaster.png\"\n",
        "inference_base=\"/tmp/inference\"\n",
        "LOGFILE=\"/tmp/inference_log\"\n",
        "XBDIR=\"/content/drive/MyDrive/xView2_baseline-master\"\n",
        "virtual_env=\"/content/drive/MyDrive/xView2_baseline-master/bin/activate\"\n",
        "localization_weights=\"/content/drive/MyDrive/localization.h5\"\n",
        "classification_weights=\"/content/drive/MyDrive/Model_data/saved_models/Model_data-saved-model-86-0.70.hdf5\"\n",
        "continue_answer=\"y\"\n",
        "\n",
        "\n",
        "# Create the output directory if it doesn't exist \n",
        "mkdir -p \"$inference_base\"\n",
        "\n",
        "if ! [ -f \"$LOGFILE\" ]; then\n",
        "    touch \"$LOGFILE\"\n",
        "fi\n",
        "\n",
        "printf \"==========\\n\" >> \"$LOGFILE\"\n",
        "echo `date +%Y%m%dT%H%M%S` >> \"$LOGFILE\"\n",
        "printf \"\\n\" >> \"$LOGFILE\"\n",
        "\n",
        "input_image=${input##*/}\n",
        "\n",
        "label_temp=\"$inference_base\"/\"${input_image%.*}\"/labels\n",
        "mkdir -p \"$label_temp\"\n",
        "\n",
        "printf \"\\n\"\n",
        "\n",
        "printf \"\\n\"\n",
        "\n",
        "# Run in inference mode\n",
        "# Because of the models _have_ to be in the correct directory, they use relative paths to find the source (e.g. \"../src\") \n",
        "# sourcing the virtual environment packages if they exist\n",
        "# this is *necessary* or all packages must be installed globally\n",
        "if [ -f  \"$virtual_env\" ]; then\n",
        "    source \"$virtual_env\"\n",
        "else\n",
        "    if [ \"$continue_answer\" = \"n\" ]; then \n",
        "        printf \"Error: cannot source virtual environment  \\n\\tDo you have all the dependencies installed and want to continue? [Y/N]: \"\n",
        "        read continue_answer \n",
        "        if [ \"$continue_answer\" == \"N\" ]; then \n",
        "               exit 2\n",
        "        fi \n",
        "    fi\n",
        "fi\n",
        "\n",
        "cd \"$XBDIR\"/model\n",
        "\n",
        "printf \"Grabbing post image file for classification\\n\"\n",
        "disaster_post_file=\"$input_post\"\n",
        "\n",
        "mkdir -p \"$inference_base\"/output_polygons\n",
        "\n",
        "printf \"Running classification\\n\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btfGMx4Pshby",
        "outputId": "6ea5921b-e968-4f45-9205-6ffe27023d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing process_data_inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile process_data_inference.py\n",
        "\n",
        "from PIL import Image\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "import cv2\n",
        "import datetime\n",
        "\n",
        "import shapely.wkt\n",
        "import shapely\n",
        "from shapely.geometry import Polygon\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def process_img(img_array, polygon_pts, scale_pct):\n",
        "    \"\"\"Process Raw Data into\n",
        "            Args:\n",
        "                img_array (numpy array): numpy representation of image.\n",
        "                polygon_pts (array): corners of the building polygon.\n",
        "            Returns:\n",
        "                numpy array: extracted polygon image from img_array.\n",
        "    \"\"\"\n",
        "\n",
        "    height, width, _ = img_array.shape\n",
        "\n",
        "    #Find the four corners of the polygon\n",
        "    xcoords = polygon_pts[:, 0]\n",
        "    ycoords = polygon_pts[:, 1]\n",
        "    xmin, xmax = np.min(xcoords), np.max(xcoords)\n",
        "    ymin, ymax = np.min(ycoords), np.max(ycoords)\n",
        "\n",
        "    xdiff = xmax - xmin\n",
        "    ydiff = ymax - ymin\n",
        "\n",
        "    #Extend image by scale percentage\n",
        "    xmin = max(int(xmin - (xdiff * scale_pct)), 0)\n",
        "    xmax = min(int(xmax + (xdiff * scale_pct)), width)\n",
        "    ymin = max(int(ymin - (ydiff * scale_pct)), 0)\n",
        "    ymax = min(int(ymax + (ydiff * scale_pct)), height)\n",
        "\n",
        "    return img_array[ymin:ymax, xmin:xmax, :]\n",
        "\n",
        "\n",
        "def process_img_poly(img_path, label_path,  output_dir, output_csv):\n",
        "    x_data = [] \n",
        "    img_obj = Image.open(img_path)\n",
        "\n",
        "    #Applies histogram equalization to image\n",
        "    img_array = np.array(img_obj)\n",
        "    #clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    #img_array = clahe.apply(img_array_pre)\n",
        "\n",
        "    #Get corresponding label for the current image\n",
        "    label_file = open(label_path)\n",
        "    label_data = json.load(label_file)\n",
        "\n",
        "    #Find all polygons in a given image\n",
        "    for feat in label_data['features']['xy']:\n",
        "\n",
        "        poly_uuid = feat['properties']['uid'] + \".png\"\n",
        "\n",
        "        # Extract the polygon from the points given\n",
        "        polygon_geom = shapely.wkt.loads(feat['wkt'])\n",
        "        polygon_pts = np.array(list(polygon_geom.exterior.coords))\n",
        "        poly_img = process_img(img_array, polygon_pts, 0.8)\n",
        "\n",
        "        # Write out the polygon in its own image\n",
        "        cv2.imwrite(output_dir + \"/\" + poly_uuid, poly_img)\n",
        "        x_data.append(poly_uuid)\n",
        "\n",
        "    data_array = {'uuid': x_data}\n",
        "    df = pd.DataFrame(data = data_array)\n",
        "    df.to_csv(output_csv)\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run Building Damage Classification Training & Evaluation')\n",
        "    parser.add_argument('--input_img',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_input\",\n",
        "                        help=\"Full path to the parent dataset directory\")\n",
        "    parser.add_argument('--label_path',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_input\",\n",
        "                        help=\"Full path to the parent dataset directory\")\n",
        "    parser.add_argument('--output_dir',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/xBD_output',\n",
        "                        help=\"Path to new directory to save images\")\n",
        "    parser.add_argument('--output_csv',\n",
        "                        required=True, \n",
        "                        metavar='/path/to/xBD_output',\n",
        "                        help=\"Path to save the csv file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    process_img_poly(args.input_img, args.label_path, args.output_dir, args.output_csv)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLQY9md2mShv"
      },
      "outputs": [],
      "source": [
        "#Using weights from pre-trained localisation model\n",
        "#If using locally trained localisation model, run Inference.py provide in bin to generate json file\n",
        "\n",
        "!python3 process_data_inference.py --input_img \"/content/drive/MyDrive/Model_data/xBD/mexico-earthquake/images/mexico-earthquake_00000002_post_disaster.png\" --label_path \"/content/drive/MyDrive/Model_data/xBD/mexico-earthquake/labels/mexico-earthquake_00000002_pre_disaster.json\" --output_dir \"/tmp/inference/output_polygons\" --output_csv \"/tmp/inference/output.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI8XJC6rtX06",
        "outputId": "f5f31cd6-4908-441f-9226-8fd2b5835a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing damage_inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile damage_inference.py\n",
        "from PIL import Image\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "import logging\n",
        "import json\n",
        "from sys import exit\n",
        "import cv2\n",
        "import datetime\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import shapely.wkt\n",
        "import shapely\n",
        "from shapely.geometry import Polygon\n",
        "from collections import defaultdict\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import ast\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Add, Input, Concatenate\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras import backend as K\n",
        "\n",
        "###\n",
        "# Loss function for ordinal loss from https://github.com/JHart96/keras_ordinal_categorical_crossentropy\n",
        "###\n",
        "def ordinal_loss(y_true, y_pred):\n",
        "    weights = K.cast(K.abs(K.argmax(y_true, axis=1) - K.argmax(y_pred, axis=1))/(K.int_shape(y_pred)[1] - 1), dtype='float32')\n",
        "    return (1.0 + weights) * keras.losses.categorical_crossentropy(y_true, y_pred )\n",
        "\n",
        "\n",
        "###\n",
        "# Generate a simple CNN\n",
        "###\n",
        "def generate_xBD_baseline_model():\n",
        "  weights = 'imagenet'\n",
        "  inputs = Input(shape=(128, 128, 3))\n",
        "\n",
        "  base_model = ResNet50(include_top=False, weights=weights, input_shape=(128, 128, 3))\n",
        "\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  x = Conv2D(32, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(128, 128, 3))(inputs)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  base_resnet = base_model(inputs)\n",
        "  base_resnet = Flatten()(base_resnet)\n",
        "\n",
        "  concated_layers = Concatenate()([x, base_resnet])\n",
        "\n",
        "  concated_layers = Dense(2024, activation='relu')(concated_layers)\n",
        "  concated_layers = Dense(524, activation='relu')(concated_layers)\n",
        "  concated_layers = Dense(124, activation='relu')(concated_layers)\n",
        "  output = Dense(3, activation='relu')(concated_layers)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# Configurations\n",
        "NUM_WORKERS = 4\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 120\n",
        "LEARNING_RATE = 0.0001\n",
        "RANDOM_SEED = 123\n",
        "LOG_DIR = '/tmp/inference/classification_log_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "\n",
        "damage_intensity_encoding = dict() \n",
        "damage_intensity_encoding[3] = 'destroyed' \n",
        "damage_intensity_encoding[2] = 'major-damage'\n",
        "damage_intensity_encoding[1] = 'minor-damage'\n",
        "damage_intensity_encoding[0] = 'no-damage'\n",
        "\n",
        "\n",
        "###\n",
        "# Creates data generator for validation set\n",
        "###\n",
        "def create_generator(test_df, test_dir, output_json_path):\n",
        "\n",
        "    gen = keras.preprocessing.image.ImageDataGenerator(\n",
        "                             rescale=1.4)\n",
        "\n",
        "    try:\n",
        "        gen_flow = gen.flow_from_dataframe(dataframe=test_df,\n",
        "                                   directory=test_dir,\n",
        "                                   x_col='uuid',\n",
        "                                   batch_size=BATCH_SIZE,\n",
        "                                   shuffle=False,\n",
        "                                   seed=RANDOM_SEED,\n",
        "                                   class_mode=None,\n",
        "                                   target_size=(128, 128))\n",
        "    except:\n",
        "        # No polys detected so write out a blank json\n",
        "        blank = {}\n",
        "        with open(output_json_path , 'w') as outfile:\n",
        "            json.dump(blank, outfile)\n",
        "        exit(0)\n",
        "\n",
        "\n",
        "    return gen_flow\n",
        "\n",
        "# Runs inference on given test data and pretrained model\n",
        "def run_inference(test_data, test_csv, model_weights, output_json_path):\n",
        "\n",
        "   model = generate_xBD_baseline_model()\n",
        "   model.load_weights(model_weights)\n",
        "\n",
        "   adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE,\n",
        "                                    beta_1=0.9,\n",
        "                                    beta_2=0.999,\n",
        "                                    epsilon=None,\n",
        "                                    decay=0.0,\n",
        "                                    amsgrad=False)\n",
        "\n",
        "\n",
        "   model.compile(loss=ordinal_loss, optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "   df = pd.read_csv(test_csv)\n",
        "\n",
        "   test_gen = create_generator(df, test_data, output_json_path)\n",
        "   test_gen.reset()\n",
        "   samples = df[\"uuid\"].count()\n",
        "\n",
        "   steps = np.ceil(samples/BATCH_SIZE)\n",
        "\n",
        "   tensorboard_callbacks = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
        "\n",
        "   predictions = model.predict_generator(generator=test_gen,\n",
        "                    callbacks=[tensorboard_callbacks],\n",
        "                    verbose=1)\n",
        "\n",
        "   predicted_indices = np.argmax(predictions, axis=1)\n",
        "   predictions_json = dict()\n",
        "   for i in range(samples):\n",
        "       filename_raw = test_gen.filenames[i]\n",
        "       filename = filename_raw.split(\".\")[0]\n",
        "       predictions_json[filename] = damage_intensity_encoding[predicted_indices[i]]\n",
        "\n",
        "   with open(output_json_path , 'w') as outfile:\n",
        "       json.dump(predictions_json, outfile)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Run Building Damage Classification Training & Evaluation')\n",
        "    parser.add_argument('--test_data',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_test_dir\",\n",
        "                        help=\"Full path to the parent dataset directory\")\n",
        "    parser.add_argument('--test_csv',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/xBD_test_csv\",\n",
        "                        help=\"Full path to the parent dataset directory\")\n",
        "    parser.add_argument('--model_weights',\n",
        "                        default=None,\n",
        "                        metavar='/path/to/input_model_weights',\n",
        "                        help=\"Path to input weights\")\n",
        "    parser.add_argument('--output_json',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/output_json\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run_inference(args.test_data, args.test_csv, args.model_weights, args.output_json)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiCBYldn8y7X",
        "outputId": "7662d4ec-3a39-457b-d283-93188be66c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-02 16:23:35.355590: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Traceback (most recent call last):\n",
            "  File \"damage_inference.py\", line 188, in <module>\n",
            "    main()\n",
            "  File \"damage_inference.py\", line 184, in main\n",
            "    run_inference(args.test_data, args.test_csv, args.model_weights, args.output_json)\n",
            "  File \"damage_inference.py\", line 126, in run_inference\n",
            "    model.load_weights(model_weights)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\", line 427, in __init__\n",
            "    swmr=swmr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\", line 190, in make_fid\n",
            "    fid = h5f.open(name, flags, fapl=fapl)\n",
            "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
            "  File \"h5py/h5f.pyx\", line 96, in h5py.h5f.open\n",
            "OSError: Unable to open file (unable to open file: name = '/content/drive/MyDrive/Model_data/saved_models/Model_data-saved-model-86-0.70.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
          ]
        }
      ],
      "source": [
        "#Generating JSON from damager classification model\n",
        "!python3 damage_inference.py --test_data \"/tmp/inference/output_polygons\" --test_csv \"/tmp/inference/output.csv\" --model_weights \"/content/drive/MyDrive/Model_data/saved_models/Model_data-saved-model-86-0.70.hdf5\" --output_json \"/tmp/inference/classification_inference.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Z4TUiHuA1L",
        "outputId": "15fa39e0-91ef-4f21-ff6b-5d6415209440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing combine_jsons.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile combine_jsons.py\n",
        "\n",
        "import json \n",
        "\n",
        "def combine_output(pred_polygons, pred_classification, output_file):\n",
        "    \"\"\"\n",
        "    :param pred_polygons: the file path to the localization inference output json  \n",
        "    :param pre_classification: the file path to the classification inference output json\n",
        "    :param output_file: the file path to store the combined json file\n",
        "    \"\"\"\n",
        "\n",
        "    # Skeleton of the json with null values \n",
        "    output_json = {\n",
        "        \"features\": {\n",
        "            \"lng_lat\": [],\n",
        "            \"xy\": []\n",
        "        }, \n",
        "        \"metadata\": {\n",
        "            \"sensor\": \"\",\n",
        "            \"provider_asset_type\": \"\",\n",
        "            \"gsd\": 0,\n",
        "            \"capture_date\": \"\", \n",
        "            \"off_nadir_angle\": 0, \n",
        "            \"pan_resolution\": 0, \n",
        "            \"sun_azimuth\": 0, \n",
        "            \"sun_elevation\": 0, \n",
        "            \"target_azimuth\": 0, \n",
        "            \"disaster\": \"\", \n",
        "            \"disaster_type\": \"\", \n",
        "            \"catalog_id\": \"\", \n",
        "            \"original_width\": 0, \n",
        "            \"original_height\": 0, \n",
        "            \"width\": 0, \n",
        "            \"height\": 0, \n",
        "            \"id\": \"\", \n",
        "            \"img_name\": \"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Open the classification json \n",
        "    with open(pred_classification) as labels:\n",
        "        label_json = json.load(labels)\n",
        "        \n",
        "        # Open the localization json \n",
        "        with open(pred_polygons) as polys:\n",
        "            poly_json = json.load(polys)\n",
        "\n",
        "            # Match UUIDs from the two jsons and combine in output_json skeleton \n",
        "            for p in poly_json['features']['xy']:\n",
        "                p['properties']['subtype'] = label_json[p['properties']['uid']]\n",
        "                output_json['features']['xy'].append(p)\n",
        "    \n",
        "    # Finally save out the combined json file \n",
        "    with open(output_file, 'w') as out: \n",
        "        json.dump(output_json, out)\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\n",
        "        \"\"\"combine_jsons.py: combines the outputs of localization and classification inference into a single output json\"\"\"\n",
        "    )\n",
        "    parser.add_argument('--polys',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/input/polygons.json',\n",
        "                        help=\"Full path to the json from polygonize.py\")\n",
        "    parser.add_argument('--classes',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/classifications.json',\n",
        "                        help=\"Full path to the json from tensor_inf.py\"\n",
        "    )\n",
        "    parser.add_argument('--output',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/pred.json',\n",
        "                        help=\"Full path to save the final single output file to\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Combining the json based off the uuid assigned at the polygonize stage\n",
        "    combine_output(args.polys, args.classes, args.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k9ni-o8AdN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc8d54d-590d-4478-97f1-7ce88170a9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"combine_jsons.py\", line 82, in <module>\n",
            "    combine_output(args.polys, args.classes, args.output)\n",
            "  File \"combine_jsons.py\", line 40, in combine_output\n",
            "    with open(pred_classification) as labels:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/inference/classification_inference.json'\n"
          ]
        }
      ],
      "source": [
        "#If running locaization locally, replace --polys with json produced by Inference.py\n",
        "!python3 combine_jsons.py --polys \"/content/drive/MyDrive/Model_data/xBD/mexico-earthquake/labels/mexico-earthquake_00000002_pre_disaster.json\" --classes \"/tmp/inference/classification_inference.json\" --output \"/tmp/inference/inference.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnNymezPuUPS",
        "outputId": "f266ee4e-b5aa-440e-a163-bc39ddfe3cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing inference_image_output.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile inference_image_output.py\n",
        "\n",
        "import json\n",
        "from shapely import wkt\n",
        "from shapely.geometry import Polygon\n",
        "import numpy as np \n",
        "from cv2 import fillPoly, imwrite\n",
        "\n",
        "def open_json(json_file_path):\n",
        "    \"\"\"\n",
        "    :param json_file_path: path to open inference json file\n",
        "    :returns: the json data dictionary of localized polygon and their classifications \n",
        "    \"\"\"\n",
        "\n",
        "    with open(json_file_path) as jf:\n",
        "        json_data = json.load(jf)\n",
        "        inference_data = json_data['features']['xy']\n",
        "        return inference_data\n",
        "\n",
        "def create_image(inference_data):\n",
        "    \"\"\"\n",
        "    :params inference_data: json data dictionary of localized polygon and their classifications\n",
        "    :returns: an numpy array of 8-bit grey scale image with polygons filled in according to the key provided\n",
        "    \"\"\"\n",
        "\n",
        "    damage_key = {'un-classified': 1, 'no-damage': 1, 'minor-damage': 2, 'major-damage': 3, 'destroyed': 4}\n",
        "\n",
        "    mask_img = np.zeros((1024,1024,1), np.uint8)\n",
        "\n",
        "    for poly in inference_data:\n",
        "        damage = poly['properties']['subtype']\n",
        "        coords = wkt.loads(poly['wkt'])\n",
        "        poly_np = np.array(coords.exterior.coords, np.int32)\n",
        "        \n",
        "        fillPoly(mask_img, [poly_np], damage_key[damage])\n",
        "    \n",
        "    return mask_img\n",
        "\n",
        "def save_image(polygons, output_path):\n",
        "    \"\"\"\n",
        "    :param polygons: np array with filled in polygons from create_image()\n",
        "    :param output_path: path to save the final output inference image\n",
        "    \"\"\"\n",
        "\n",
        "    # Output the filled in polygons to an image file\n",
        "    imwrite(output_path, polygons)\n",
        "  \n",
        "def create_inference_image(json_input_path, image_output_path):\n",
        "    \"\"\"\n",
        "    :param json_input_path: Path to output inference json file\n",
        "    :param image_outut_pat: Path to save the final inference image\n",
        "    \"\"\"\n",
        "\n",
        "    # Getting the inference data from the localization and classification \n",
        "    inference_data = open_json(json_input_path)\n",
        "\n",
        "    # Filling in the polygons and readying the image format \n",
        "    polygon_array = create_image(inference_data)\n",
        "\n",
        "    # Saving the image to the desired location\n",
        "    save_image(polygon_array, image_output_path)\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\n",
        "        \"\"\"inference_image_output.py: Takes the inference localization and classification final outputs in json from and outputs an image ready to be scored based off the challenge parameters\"\"\")\n",
        "    parser.add_argument('--input',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/final/inference.json',\n",
        "                        help=\"Full path to the final inference json\")\n",
        "    parser.add_argument('--output',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/inference.png',\n",
        "                        help=\"Full path to save the image to\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Creating the scoring image\n",
        "    create_inference_image(args.input, args.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHlTgar9FSiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9afdd5-6018-4835-a67a-ae97ceae50c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"inference_image_output.py\", line 81, in <module>\n",
            "    create_inference_image(args.input, args.output)\n",
            "  File \"inference_image_output.py\", line 54, in create_inference_image\n",
            "    inference_data = open_json(json_input_path)\n",
            "  File \"inference_image_output.py\", line 14, in open_json\n",
            "    with open(json_file_path) as jf:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/inference/inference.json'\n"
          ]
        }
      ],
      "source": [
        "!python3 inference_image_output.py --input \"/tmp/inference/inference.json\" --output \"/content/drive/MyDrive/Model_data/inference-output/baseline_output002.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5CFwd3oRXO4",
        "outputId": "163ae990-8597-4e21-f572-a207347973b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing display_image.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile display_image.py\n",
        "\n",
        "import numpy\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "def get_image(input_image, image_output):\n",
        "  image = cv2.imread(input_image)\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "  cv2.imshow('Original image',image)\n",
        "  cv2.imshow('Gray image', gray)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Convert Image to RGBA')\n",
        "    parser.add_argument('--input_image',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/inference-output/image.png\",\n",
        "                        help=\"Full path to the inference output image to be converted\")\n",
        "    parser.add_argument('--image_output',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/output_directory\",\n",
        "                        help=\"Full path to the ilocation of the output image\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    get_image(args.input_image, args.image_output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p3ptPXMUJHP",
        "outputId": "ef8cc890-837b-4852-b70e-a2b59604d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"display_image.py\", line 31, in <module>\n",
            "    get_image(args.input_image, args.image_output)\n",
            "  File \"display_image.py\", line 9, in get_image\n",
            "    gray = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
            "cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = (cv::impl::<unnamed>::SizePolicy)2u; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n",
            "> Invalid number of channels in input image:\n",
            ">     'VScn::contains(scn)'\n",
            "> where\n",
            ">     'scn' is 3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 display_image.py --input_image \"/content/drive/MyDrive/Model_data/inference-output/baseline_output002.png\" --image_output \"/content/drive/MyDrive/Model_data/inference-output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEGzu3zsI9oZ",
        "outputId": "ab315cad-0f96-42fa-a19d-557a94808b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up\n",
            "/bin/bash: line 3: : No such file or directory\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "#Removing temporary directories\n",
        "%%shell\n",
        "printf \"Cleaning up\\n\"\n",
        "rm -rf \"$inference_base\"\n",
        "\n",
        "printf \"==========\\n\" >> \"$LOGFILE\"\n",
        "printf \"Done!\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl94ywxH7JqK"
      },
      "source": [
        "# BIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwspR-1iuyNz",
        "outputId": "e0f5e14c-d5c4-41bf-9edd-ac2a4be4b33c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting inference.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile inference.py\n",
        "\n",
        "import resource\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/xView2_baseline-master/spacenet/src/models')\n",
        "\n",
        "import segmentation_cpu\n",
        "from segmentation_cpu import SegmentationModel as Model\n",
        "from os import path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from uuid import uuid4\n",
        "import json\n",
        "from imantics import Polygons, Mask\n",
        "from simplification.cutil import simplify_coords_vwp\n",
        "\n",
        "def create_wkt(polygon):\n",
        "    \"\"\"\n",
        "    :param polygon: a single polygon in the format [(x1,y1), (x2,y2), ...]\n",
        "    :returns: a wkt formatted string ready to be put into the json \n",
        "    \"\"\"\n",
        "    wkt = 'POLYGON (('\n",
        "\n",
        "    for coords in polygon:\n",
        "        wkt += \"{} {},\".format(coords[0], coords[1])\n",
        "\n",
        "    wkt = wkt[:-1] + '))'\n",
        "\n",
        "    return wkt\n",
        "\n",
        "\n",
        "def create_json(adjusted_polygons):\n",
        "    \"\"\"\n",
        "    :param polygons: list of polygons in the format [(x1,y1), (x2,y2), ...]\n",
        "    :returns: json with found and adjusted polygon pixel x,y values in WKT format\n",
        "    \"\"\"\n",
        "    # Create a blank json that matched the labeler provided jsons with null or default values\n",
        "    output_json = {\n",
        "        \"features\": {\n",
        "            \"lng_lat\": [],\n",
        "            \"xy\": []\n",
        "        }, \n",
        "        \"metadata\": {\n",
        "            \"sensor\": \"\",\n",
        "            \"provider_asset_type\": \"\",\n",
        "            \"gsd\": 0,\n",
        "            \"capture_date\": \"\", \n",
        "            \"off_nadir_angle\": 0, \n",
        "            \"pan_resolution\": 0, \n",
        "            \"sun_azimuth\": 0, \n",
        "            \"sun_elevation\": 0, \n",
        "            \"target_azimuth\": 0, \n",
        "            \"disaster\": \"\", \n",
        "            \"disaster_type\": \"\", \n",
        "            \"catalog_id\": \"\", \n",
        "            \"original_width\": 0, \n",
        "            \"original_height\": 0, \n",
        "            \"width\": 0, \n",
        "            \"height\": 0, \n",
        "            \"id\": \"\", \n",
        "            \"img_name\": \"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Using a lambda function to place the WKT string in the list of polygons \n",
        "    polygon_template = lambda poly, uuid: {\n",
        "        'properties': {\n",
        "            'feature_type': 'building',\n",
        "            'uid': uuid\n",
        "        },\n",
        "        'wkt': poly\n",
        "    }\n",
        "\n",
        "    # For each adjusted polygon add the wkt for the polygon points\n",
        "    for polygon in adjusted_polygons:\n",
        "        wkt = create_wkt(polygon)\n",
        "        uuid = gen_uuid()\n",
        "        poly = polygon_template(wkt, uuid)\n",
        "        output_json['features']['xy'].append(poly)\n",
        "\n",
        "    return output_json\n",
        "\n",
        "def gen_uuid():\n",
        "    return str(uuid4())\n",
        "\n",
        "def inference(image, score, output_file):\n",
        "    building_score = score[1]\n",
        "    \n",
        "    building_mask_pred = (np.argmax(score, axis=0) == 1)\n",
        "    polygons = Mask(building_mask_pred).polygons()\n",
        "    \n",
        "    new_predictions = []\n",
        "    \n",
        "    for poly in polygons:\n",
        "        if len(poly) >= 3:\n",
        "            f = poly.reshape(-1, 2)\n",
        "            simplified_vw = simplify_coords_vwp(f, .3)\n",
        "            if len(simplified_vw) > 2:\n",
        "                    mpoly = []\n",
        "                    # Rebuilding the polygon in the way that PIL expects the values [(x1,y1),(x2,y2)]\n",
        "                    for i in simplified_vw:\n",
        "                        mpoly.append((i[0], i[1]))\n",
        "                    # Adding the first point to the last to close the polygon\n",
        "                    mpoly.append((simplified_vw[0][0], simplified_vw[0][1]))\n",
        "                    new_predictions.append(mpoly)\n",
        "            \n",
        "    # Creating the json with the predicted and then adjusted polygons\n",
        "    output_json = create_json(new_predictions)\n",
        "    \n",
        "    with open(output_file, 'w') as out_file:\n",
        "        json.dump(output_json, out_file)\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "    import argparse\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\n",
        "        \"\"\"inference.py: takes an image and creates inferred polygons json off the VW algorithm and the unet model predictions\"\"\"\n",
        "    )\n",
        "    parser.add_argument('--input',\n",
        "                        required=True,\n",
        "                        metavar='/path/to/input/image.png')\n",
        "    parser.add_argument(\n",
        "        '--weights',\n",
        "        required=True,\n",
        "        metavar='/full/path/to/mode_iter_XXXX',\n",
        "        help=\"Must be the output to a unet model weights trained for xView2\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--mean',\n",
        "        required=True,\n",
        "        metavar='/full/path/to/mean.npy',\n",
        "        help=\"a numpy data structure file that is the mean of the training images (found by running ./src/features/compute_mean.py)\"\n",
        "    )\n",
        "    parser.add_argument('--output',\n",
        "                        required=True,\n",
        "                        metavar=\"/path/to/output/file.json\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load trained model\n",
        "    # Modify the paths based on your trained model location if needed.\n",
        "    mean = np.load(args.mean)\n",
        "    model = Model(args.weights, mean)\n",
        " \n",
        "    image = np.array(Image.open(args.input))\n",
        "    score = model.apply_segmentation(image)\n",
        "    inference(image, score, args.output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3UG-PZaOg4C"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "set -euo pipefail\n",
        "\n",
        "# this function is called when Ctrl-C is sent\n",
        "function trap_ctrlc ()\n",
        "{\n",
        "    # perform cleanup here\n",
        "    echo \"Ctrl-C or Error caught...performing clean up check /tmp/inference.log\"\n",
        "\n",
        "    if [ -d /tmp/inference ]; then\n",
        "           rm -rf /tmp/inference\n",
        "    fi\n",
        "\n",
        "    exit 99\n",
        "}\n",
        "\n",
        "# initialise trap to call trap_ctrlc function\n",
        "# when signal 2 (SIGINT) is received\n",
        "trap \"trap_ctrlc\" 2 9 13 3\n",
        "\n",
        "help_message () {\n",
        "        printf \"${0}: Runs the polygonization in inference mode\\n\\t-x: path to xview-2 repository\\n\\t-i: /full/path/to/input/pre-disaster/image.png\\n\\t-p: /full/path/to/input/post-disaster/image.png\\n\\t-o: /path/to/output.png\\n\\t-l: path/to/localization_weights\\n\\t-c: path/to/classification_weights\\n\\t-e /path/to/virtual/env/activate\\n\\t-y continue with local environment and without interactive prompt\\n\\n\"\n",
        "}\n",
        "\n",
        "input=\"/content/drive/MyDrive/mexico_pre/mexico-earthquake_00000002_pre_disaster.png\"\n",
        "input_post=\"/content/drive/MyDrive/mexico_post/mexico-earthquake_00000002_post_disaster.png\"\n",
        "inference_base=\"/tmp/inference\"\n",
        "LOGFILE=\"/tmp/inference_log\"\n",
        "XBDIR=\"/content/drive/MyDrive/xView2_baseline-master\"\n",
        "virtual_env=\"/content/drive/MyDrive/xView2_baseline-master/bin/activate\"\n",
        "localization_weights=\"/content/drive/MyDrive/localization.h5\"\n",
        "classification_weights=\"/content/drive/MyDrive/Model_data/saved_models/Model_data-saved-model-86-0.70.hdf5\"\n",
        "continue_answer=\"y\"\n",
        "\n",
        "\n",
        "# Create the output directory if it doesn't exist \n",
        "mkdir -p \"$inference_base\"\n",
        "\n",
        "if ! [ -f \"$LOGFILE\" ]; then\n",
        "    touch \"$LOGFILE\"\n",
        "fi\n",
        "\n",
        "printf \"==========\\n\" >> \"$LOGFILE\"\n",
        "echo `date +%Y%m%dT%H%M%S` >> \"$LOGFILE\"\n",
        "printf \"\\n\" >> \"$LOGFILE\"\n",
        "\n",
        "input_image=${input##*/}\n",
        "\n",
        "label_temp=\"$inference_base\"/\"${input_image%.*}\"/labels\n",
        "mkdir -p \"$label_temp\"\n",
        "\n",
        "printf \"\\n\"\n",
        "\n",
        "printf \"\\n\"\n",
        "\n",
        "# Run in inference mode\n",
        "# Because of the models _have_ to be in the correct directory, they use relative paths to find the source (e.g. \"../src\") \n",
        "# sourcing the virtual environment packages if they exist\n",
        "# this is *necessary* or all packages must be installed globally\n",
        "if [ -f  \"$virtual_env\" ]; then\n",
        "    source \"$virtual_env\"\n",
        "else\n",
        "    if [ \"$continue_answer\" = \"n\" ]; then \n",
        "        printf \"Error: cannot source virtual environment  \\n\\tDo you have all the dependencies installed and want to continue? [Y/N]: \"\n",
        "        read continue_answer \n",
        "        if [ \"$continue_answer\" == \"N\" ]; then \n",
        "               exit 2\n",
        "        fi \n",
        "    fi\n",
        "fi\n",
        "\n",
        "cd \"$XBDIR\"/spacenet/inference/\n",
        "\n",
        "# Quietly running the localization inference to output a json with the predicted polygons from the supplied input image\n",
        "printf \"Running localization\\n\"\n",
        "python3 ./inference.py --input \"$input\" --weights \"$localization_weights\" --mean \"$XBDIR\"/weights/mean.npy --output \"$label_temp\"/\"${input_image%.*}\".json >> \"$LOGFILE\" 2>&1\n",
        "\n",
        "printf \"\\n\" >> \"$LOGFILE\"\n",
        "\n",
        "# Classification inferences start below\n",
        "cd \"$XBDIR\"/model\n",
        "\n",
        "# Replace the pre image here with the post\n",
        "# We need to do this so the classification inference pulls the images from the post \n",
        "# Since post is where the damage occurs\n",
        "printf \"Grabbing post image file for classification\\n\"\n",
        "disaster_post_file=\"$input_post\"\n",
        "\n",
        "mkdir -p \"$inference_base\"/output_polygons\n",
        "\n",
        "printf \"Running classification\\n\" \n",
        "\n",
        "# Extracting polygons from post image I THINK ITS BROKEN HERE!!!!!! not generating output polygons?????\n",
        "python3 ./process_data_inference.py --input_img \"$disaster_post_file\" --label_path \"$label_temp\"/\"${input_image%.*}\".json --output_dir \"$inference_base\"/output_polygons --output_csv \"$inference_base\"/output.csv >> \"$LOGFILE\" 2>&1\n",
        "\n",
        "# Classifying extracted polygons\n",
        "python3 ./damage_inference.py --test_data \"$inference_base\"/output_polygons --test_csv \"$inference_base\"/output.csv --model_weights \"$classification_weights\" --output_json /tmp/inference/classification_inference.json >> \"$LOGFILE\" 2>&1\n",
        "\n",
        "printf \"\\n\" >> \"$LOGFILE\"\n",
        "\n",
        "# Combining the predicted polygons with the predicted labels, based off a UUID generated during the localization inference stage  \n",
        "printf \"Formatting json and scoring image\\n\"\n",
        "python3 \"$XBDIR\"/utils/combine_jsons.py --polys \"$label_temp\"/\"${input_image%.*}\".json --classes /tmp/inference/classification_inference.json --output \"$inference_base/inference.json\" >> \"$LOGFILE\" 2>&1\n",
        "printf \"\\n\" >> \"$LOGFILE\"\n",
        "\n",
        "# Transforming the inference json file to the image required for scoring\n",
        "printf \"Finalizing output file\" \n",
        "python3 \"$XBDIR\"/utils/inference_image_output.py --input \"$inference_base\"/inference.json --output \"$output_file\"  >> \"$LOGFILE\" 2>&1\n",
        "\n",
        "#Cleaning up by removing the temporary working directory we created\n",
        "printf \"Cleaning up\\n\"\n",
        "rm -rf \"$inference_base\"\n",
        "\n",
        "printf \"==========\\n\" >> \"$LOGFILE\"\n",
        "printf \"Done!\\n\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "wy49hRqJfpUt",
        "gGo5oiP70NGm",
        "4ZpFXMiO0YJz",
        "IvHGVVmYw_xP",
        "Nl94ywxH7JqK"
      ],
      "name": "xBD_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}